{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Module 3, Lab 2 - Association\n=============================\n\nIn this lab, we will examine how to analyze data for a correlation. Note\nthat a detailed dive into correlational and regression-based research is\ngiven in Module 5. However, a brief overview is provided here. I focus\non correlation because it is the simplest way to make an association\nclaim, but as we saw in the online lesson, actually the correct analysis\ndepends on your data (continuous, discrete, normal vs non-normal, etc.).\nThus, a full illustration of all association techniques would take many,\nmany labs. I focus on correlation here.\n___\nMódulo 3, Laboratório 2 - Associação\n====\nNeste laboratório, examinaremos como analisar dados para uma correlação. Observe que um mergulho detalhado na pesquisa correlacional e baseada em regressão é fornecido no Módulo 5. No entanto, uma breve visão geral é fornecida aqui. Eu me concentro na correlação porque é a maneira mais simples de fazer uma afirmação de associação, mas, como vimos na lição online, na verdade, a análise correta depende dos seus dados (contínuos, discretos, normais vs não normais, etc.). Portanto, uma ilustração completa de todas as técnicas de associação exigiria muitos, muitos laboratórios. Eu me concentro na correlação aqui.\n___\n\n\nIn this example, you are analyzing customer loyalty data. Your\norganization uses three measures of loyalty, and you wish to test them\nout. (To avoid discussions of popular real measures, we will name these\n`loytalty1`, `loyalty2`, and `loyalty3`).\n\nNote that this lab uses the `ggplot2` package for data visualization and\nthe `psych` package for correlation testing. I also assume you are\nfamiliar with `ggplot2`. As an alternative to the `psych` tools, we can\nalso use the `Hmisc` package for correlation testing.\n___\nNeste exemplo, você está analisando dados de fidelidade do cliente. Sua organização usa três medidas de lealdade e você deseja testá-las. (Para evitar discussões sobre medidas reais populares, vamos chamá-las de lealdade1, lealdade2 e lealdade3).\n\nObserve que este laboratório usa o pacote ggplot2 para visualização de dados e o pacote psic para teste de correlação. Também presumo que você esteja familiarizado com o ggplot2. Como alternativa às ferramentas psicológicas, também podemos usar o pacote Hmisc para teste de correlação.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#### LOAD PACKAGES ####\n## Use inline magic command so plots appear in the data frame\n%matplotlib inline\n\n## Next the packages\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as ss\nimport math",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You load the data from the CSV file in the github folder for this lab:\n___\nVocê carrega os dados do arquivo CSV na pasta github deste laboratório:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#### LOAD DATA ####\ndat = pd.read_csv(\"datasets/loyaltydata.csv\")",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You inspect the data:\n___\n\nVocê inspeciona os dados:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(dat.columns)\n\ndat.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "There is an ID variable shown as `Unnamed` and scores on a loyalty measures named\n`loyalty1` through `loyalty3`. \n\n****\n**Note:** You are not yet familiar with the scaling of these measures.\n***\n\nThe first thing to do is to explore the variables. The Pandas `describe()` method.\n\n___\nHá uma variável de ID mostrada como Sem nome e pontuações em medidas de lealdade denominadas lealdade1 até lealdade3.\n\nObservação: você ainda não está familiarizado com a escala dessas medidas.\n\nA primeira coisa a fazer é explorar as variáveis. O método Pandas describe ().",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dat.describe()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This gives you a sense as to the range and scaling of each loyalty\nmeasure.\n\nImagine that each loyalty measure was in common use. You might want to\nknow whether they are highly correlated. We can compute correlations\nbetween variables with the Pandas `corr()` method. A subset of the data frame is taken (using the outer `[]` operator) by providing a list (the inner `[]`) of column names.\n___\n\nIsso lhe dá uma noção do alcance e da escala de cada medida de fidelidade.\n\nImagine que cada medida de lealdade fosse de uso comum. Você pode querer saber se eles são altamente correlacionados. Podemos calcular correlações entre variáveis ​​com o método Pandas corr (). Um subconjunto do quadro de dados é obtido (usando o operador externo []) fornecendo uma lista (o interno []) de nomes de coluna.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dat[['loyalty1', 'loyalty2', 'loyalty3']].corr()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This difficult to read. Let's use the Pandas `round()` method:\n___\n\nÉ difícil de ler. Vamos usar o método round () do Pandas:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "corr_mat = dat[['loyalty1', 'loyalty2', 'loyalty3']].corr().round(2)\ncorr_mat",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We see here that the variables are *not* highly correlated with each\nother. This is a potential problem.\n\nA brief refresher: correlations range between zero (no association\nbetween variables) and 1.0 (a one-to-one association). They can also be\npositive (as one variable increases, so does the other) or negative (as\none variable increases, the other decreases).\n___\nVemos aqui que as variáveis ​​não são altamente correlacionadas entre si. Este é um problema potencial.\n\nUma breve atualização: as correlações variam entre zero (nenhuma associação entre as variáveis) e 1,0 (uma associação um-para-um). Eles também podem ser positivos (conforme uma variável aumenta, a outra aumenta) ou negativos (conforme uma variável aumenta, a outra diminui).\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The statistician Jacob Cohen suggested the following guidelines:  \n\n<pre>\n| # | Correlation |  Meaning   |\n|---|-------------|------------|\n| 1 |  0.0 - 0.1  | Negligible |\n| 2 |  0.1 - 0.3  |   Small    |\n| 3 |  0.3 - 0.5  |   Medium   |\n| 4 |    0.5 +    |   Large    |\n</pre>\n\nHowever, given that they are all ostensibly measuring the same thing,\nloyalty, we should expect much higher correlations (.7-.9).\n\nWe can also easily visualize this correlation with using the `heatmap` function from the Python Seaborn package. Seaborn is a sophisticated statistical charting package. \n\n___\nO estatístico Jacob Cohen sugeriu as seguintes diretrizes:\n\nNo entanto, dado que todos eles estão medindo ostensivamente a mesma coisa, lealdade, devemos esperar correlações muito mais altas (0,7-0,9).\n\nTambém podemos visualizar facilmente essa correlação usando a função de mapa de calor do pacote Python Seaborn. Seaborn é um pacote sofisticado de gráficos estatísticos.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.heatmap(corr_mat, vmax=1.0) \nplt.title('Correlation matrix for loyalty features')\nplt.yticks(rotation='horizontal')\nplt.xticks(rotation='vertical')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Next, you will create scatter plots of each pairwise combination of loyalty variables. The code uses the `lmplot` function from Seaborn. Jitter on both the x and y axes along with high point transparency are used to help deal with over-plotting. Notice that the transparency argument, `alpha`, must be passed to the underling Matplotlib in a dictionary called `scatter_kws`. \n___\n\nA seguir, você criará gráficos de dispersão de cada combinação de pares de variáveis ​​de lealdade. O código usa a função lmplot da Seaborn. Jitter em ambos os eixos xey junto com transparência de ponto alto são usados ​​para ajudar a lidar com overplotting. Observe que o argumento de transparência, alfa, deve ser passado para o Matplotlib subjacente em um dicionário chamado scatter_kws.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.lmplot(\"loyalty1\", \"loyalty2\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "sns.lmplot(\"loyalty1\", \"loyalty3\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "sns.lmplot(\"loyalty2\", \"loyalty3\", dat, x_jitter=.15, y_jitter=.15, scatter_kws={'alpha':0.2}, fit_reg = False)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-8-1.png)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-8-3.png)\nAll of the graphs look about the same. It is always good to inspect the\nplots, as we know that non-linearity can weaken our correlations. Here,\nwe see evidence that each measure is correlated linearly; the\nassociations are simply underwhelming.\n___\nTodos os gráficos têm a mesma aparência. É sempre bom inspecionar os gráficos, pois sabemos que a não linearidade pode enfraquecer nossas correlações. Aqui, vemos evidências de que cada medida é correlacionada linearmente; as associações são simplesmente desanimadoras.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "![](Mod3_Lab2_-_Association_files/figure-markdown_strict/unnamed-chunk-9-1.png)\n\nWe can easily compute the confidence intervals of these correlation coefficients. However, this requires a few steps (don't worry if you don't follow this completely:   \n1. Transform the correlation from the initial space which we call r to a transformed space z. The distribution of errors is Normal in this transformed space. \n2. Compute the CI in the transformed space.\n3. Transform back to the original space.\n\n___\n\nPodemos calcular facilmente os intervalos de confiança desses coeficientes de correlação. No entanto, isso requer algumas etapas (não se preocupe se você não seguir completamente:\n\n1. Transforme a correlação do espaço inicial que chamamos de r em um espaço transformado z. A distribuição de erros é Normal neste espaço transformado.\n2. Calcule o IC no espaço transformado.\n3. Transforme-se de volta ao espaço original.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def r_z(r):\n    return math.log((1 + r) / (1 - r)) / 2.0\n\ndef z_r(z):\n    e = math.exp(2 * z)\n    return((e - 1) / (e + 1))\n\ndef r_conf_int(r, alpha, n):\n    # Transform r to z space\n    z = r_z(r)\n    # Compute standard error and critcal value in z\n    se = 1.0 / math.sqrt(n - 3)\n    z_crit = ss.norm.ppf(1 - alpha/2)\n\n    ## Compute CIs with transform to r\n    lo = z_r(z - z_crit * se)\n    hi = z_r(z + z_crit * se)\n    return (lo, hi)\n\nprint('\\nFor loyalty1 vs. loyalty2')\ncorr_mat = np.array(corr_mat)\nconf_ints = r_conf_int(corr_mat[1,0], 0.05, 1000)\nprint('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[1,0], conf_ints[0], conf_ints[1]))\nprint('\\nFor loyalty1 vs. loyalty3')\nconf_ints = r_conf_int(corr_mat[2,0], 0.05, 1000)\nprint('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[2,0], conf_ints[0], conf_ints[1]))\nprint('\\nFor loyalty2 vs. loyalty3')\nconf_ints = r_conf_int(corr_mat[2,1], 0.05, 1000)\nprint('Correlation = %4.3f with CI of %4.3f to %4.3f' % (corr_mat[2,1], conf_ints[0], conf_ints[1]))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You can see that the CIs of all the correlation coefficients are relatively small compared to the correlation coefficients. This indicates that these coefficients are statistically significant.\n___\n\nVocê pode ver que os ICs de todos os coeficientes de correlação são relativamente pequenos em comparação com os coeficientes de correlação. Isso indica que esses coeficientes são estatisticamente significativos.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "What Claims Can We Make?\n========================\n\nHere, we can make the following claims: each of these variables is\ncorrelated with each other, but in reality, the correlations are weaker\nthan you would hope them to be. In this case, we can have a series of\nconversations about whether these measures of loyalty are assessing\ndifferent things, whether there are actually different kinds of customer\nloyalty, or whether the measures are not of high quality. Regardless,\nthere appears to *not* be a large association between our measures of\nloyalty. In fact, using 95% CIs, we found that we had fairly precise\nestimate of our correlations: they are not strong. This raises large\nimplications for our organization as it considers using these measures.\n\n___\n\nQue afirmações podemos fazer?\n====\nAqui, podemos fazer as seguintes afirmações: cada uma dessas variáveis ​​está correlacionada umas com as outras, mas na realidade, as correlações são mais fracas do que você esperaria que fossem. Nesse caso, podemos ter uma série de conversas sobre se essas medidas de fidelidade estão avaliando coisas diferentes, se realmente existem diferentes tipos de fidelidade do cliente ou se as medidas não são de alta qualidade. Independentemente disso, parece não haver uma grande associação entre nossas medidas de lealdade. Na verdade, usando ICs de 95%, descobrimos que tínhamos uma estimativa bastante precisa de nossas correlações: elas não são fortes. Isso levanta grandes implicações para nossa organização, pois ela considera o uso dessas medidas.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}