{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Module 2, Lab 2: *p*-values\n===========================\n\nIn this lab, we will explore how the basics of null hypothesis\nsignificance testing work. Although you may have examined this in a\nprevious course, we will review the concepts of *p*-values and tests of\nstatistical significance with an emphasis on their application in\nresearch.\n\n___\nMódulo 2, Laboratório 2: valores p\n===========================\n\nNeste laboratório, exploraremos como os conceitos básicos trabalho de teste de significância de hipótese nula. Embora você possa ter examinado isso em um No curso anterior, revisaremos os conceitos de valores p e testes de significância estatística com ênfase em sua aplicação em pesquisa.\n___\n\n\nThe Null Hypothesis\n===================\n\nFirst, we briefly review what the null hypothesis is. Recall from the\nprevious lab that the results that come from samples are only mere\nestimates of the population. Because they are estimates, the statistics\nthey produce will differ somewhat from their population counterparts.\nFor example, the correlation between engagement in a sample may be *r* =\n.2 even when the correlation between those same variables in the\npopulation is something smaller, such as .10 or even 0. This can cause\napparent relationships and effects to appear in *samples* when none in\nfact exists in the population. This idea--that the effect/association is\n*zero* in the population--is called the null hypothesis. By implication,\nany effect/association seen in the *sample* must be entirely due to the\nrandom chance of \"sampling error.\" In other words, the null hypothesis\nclaims that the sample result is a random fluke.\n___\nA hipótese nula¶\n===================\n\nPrimeiro, revisamos brevemente qual é a hipótese nula. Lembre-se do laboratório anterior que os resultados provenientes de amostras são apenas meros estimativas da população. Por serem estimativas, as estatísticas eles produzem diferem um pouco de suas contrapartes populacionais. Por exemplo, a correlação entre o envolvimento em uma amostra pode ser r = .2 mesmo quando a correlação entre essas mesmas variáveis ​​no população é algo menor, como 0,10 ou até 0. Isso pode causar aparentes relações e efeitos a aparecer em amostras quando nenhuma fato existe na população. Esta ideia - que o efeito / associação é zero na população - é chamada de hipótese nula. Por implicação, qualquer efeito / associação observado na amostra deve ser inteiramente devido à chance aleatória de \"erro de amostragem\". Em outras palavras, a hipótese nula afirma que o resultado da amostra é um acaso aleatório.\n___\nLet's explore an application of this. Imagine we want to compare males\nand females in terms of their interest in a given product. Imagine, for\na moment, that *the two groups have identical interest* (in the\npopulation)...that is, there is no difference between the groups.\nNevertheless, if we take a sample of males and a sample of females, the\nerror in our estimations will cause a difference to appear.\n\nImagine that *both* males and females had an interest level averaging at\n5, with a standard deviation of 3.\n\n___\nVamos explorar uma aplicação disso. Imagine que queremos comparar homens e mulheres em termos de interesse em um determinado produto. Imagine, por por um momento, que os dois grupos têm interesse idêntico (no população) ... ou seja, não há diferença entre os grupos. No entanto, se coletarmos uma amostra de homens e uma amostra de mulheres, o um erro nas nossas estimativas fará com que apareça uma diferença.\n\nImagine que ambos homens e mulheres tiveram um nível de interesse médio de 5, com um desvio padrão de 3\n___",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# set seed to make random number generation reproducible\nimport numpy as np\nimport numpy.random as nr\nnr.seed(51120122)\n\n#collect a sample of 100 males\nmales = nr.normal(5, 3, 100)\n\n#collect a sample of 100 females\nfemales = nr.normal(5, 3, 100)\n\nprint(np.mean(males))\nprint(np.mean(females))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "text": "5.171234200421537\n5.898998940622083\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "We see here that our two groups have different sample results. Let's see\nhow large the difference is:\n___\nVemos aqui que nossos dois grupos têm resultados de amostra diferentes. Vamos ver\nquão grande é a diferença:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.mean(males)-np.mean(females)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "-0.7277647402005458"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "We see here that the females are almost 3/4 of a point higher than the\nmales. If you saw this data in an organization where you were working,\nyou might be tempted to think you'd discovered a female preference for\nyour product. However, in fact, we *know* in this case that this is\nnonsense as we *know* (because we wrote the Python code simulating this data)\nthat *both* groups were random samples from a population with a mean of\n5 and a standard deviation of 3. If their means are both 5.0 (exactly)\nin the population, why did the females score higher in the samples? It's\nsimple: sampling error. That is, the difference is **entirely** due to\nrandom error in the samples, not any real difference in the population.\nWe have discovered a fluke in some sample data, nothing more.\n___\nVemos aqui que as fêmeas são quase 3/4 de um ponto mais alto que o machos. Se você viu esses dados em uma organização em que estava trabalhando, você pode ficar tentado a pensar que descobriu uma preferência feminina por seu produto. No entanto, de fato, sabemos neste caso que isso é absurdo como conhecemos (porque escrevemos o código Python simulando esses dados) que ambos grupos eram amostras aleatórias de uma população com média de 5 e um desvio padrão de 3. Se suas médias forem ambas 5,0 (exatamente) na população, por que as fêmeas pontuaram mais nas amostras? Está simples: erro de amostragem. Ou seja, a diferença é inteiramente devido a erro aleatório nas amostras, nenhuma diferença real na população. Descobrimos um acaso em alguns dados de amostra, nada mais.\n___\nThis is a case of the \"null hypothesis.\" In this case, the means are\n*equal* in the population. We write the null hypothesis as\n*H*<sub>0</sub> and it is always a statement that the size of the effect\nin the population is zero. In this case, we are testing the difference\nbetween the averages ($\\mu ' s$), stating that the *difference between\nthem is zero*:\n\n$$H_0 :\\ \\mu_{male} - \\mu_{female} = 0$$\n\nHowever, to reiterate what we saw above, *when we looked at our\nsamples,* we saw there was a difference:\n___\n\n\nEste é um caso da \"hipótese nula\". Nesse caso, as médias são\n*iguais* na população. Escrevemos a hipótese nula como\n*H* <sub> 0 </sub> e é sempre uma declaração de que o tamanho do efeito\nna população é zero. Nesse caso, estamos testando a diferença\nentre as médias ($\\mu ' s$), declarando que a * diferença entre\neles é zero *:\n\n$$H_0 :\\ \\mu_{male} - \\mu_{female} = 0$$\n\nNo entanto, para reiterar o que vimos acima, *quando analisamos nossa\namostras* vimos que havia uma diferença:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.mean(males)-np.mean(females)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "-0.7277647402005458"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "So, in conclusion, the null hypothesis says that whatever effect you are\nstudying is *zero* in the population and *your sample results are due to\nrandom chance.*\n\nThis possibility looms ominously over every research finding based on\nsamples data. How do we know that the effects we trust every day (the\neffect of medicine, tested leadership practices, etc.) are real and not\njust flukes due to random sampling error? We need to find a way to test\nthe null hypothesis and see if we can reject this possibility.\n\nNull Hypothesis Significance Test: The *p*-Value\n================================================\n\nTo test the null hypothesis, we simply ask: *if the null hypothesis were\ntrue, what percentage of the time would I get this result this large?*\nThe answer to that question is called a *p*-value.\n\nThere is a lot of confusion about *p*-values, so let's review:\n\n-   *p*-values represent how often you could get a result as big as you\n    did *if the null were true*\n-   *p*-values therefore represent how easy/hard it would be to get a\n    result by chance\n-   *p*-values do **not** tell you the probability that the result is\n    due to chance; only the probability of seeing *your result* if the\n    null were true\n-   If the *p*-value for a result is small, it would be rare to get that\n    result by chance (i.e., if the null were true)\n-   If the *p*-value for a result is large, it would be common to get\n    that result by chance (i.e., if the null were true)\n-   Conclusion: the *p*-value is a measure of \"incompatibility\" between\n    your result and the null. If the *p*-value is small, one of the two\n    (the data, or the null) is likely wrong. We opt to trust our data\n    and reject the null.\n\nTo be clear: the *p*-value is a backwards way of testing the null\nhypothesis. We would love to know the *probability* that the null\nhypothesis is true--the probability that the results *are* due to\nchance--but we cannot know that. You will often hear the *p*-value\ndescribed this way, but that is **very wrong**.\n\nSo, to repeat, the *p-value states the probability of getting **your\nresult** if the null is true*. It is essentially a statement of\nincompatibility between your data and the null. A small *p*-value\n(typically, less than 5% or \"&lt; . 05\") tells you that the data and\nnull are highly incompatible. Since you did in fact observe the data,\nyou conclude the null hypothesis is false. This is the only use for the\n*p*-value.\n\nWhere do *p*-Values Come From?\n==============================\n\nWhere does a *p*-value come from? Every data situation is different, but\nthe process in so-called \"frequentist\" statistics is always the same\n\n1.  Observe data and examine result\n2.  Compute the appropriate \"test statistic\" for that result (e.g., *t*\n    test, *z* test, *χ*<sup>2</sup> test, *F* test, *q* test etc.).\n3.  Observe how often you could get the observed test statistic if the\n    null hypothesis was true. This is the *p*-value\n4.  If the *p*-value is less than .05, declare the result \"significant\"\n    and reject the null hypothesis\n\nLet's see this in action. For this example, I will use a \"one-sample\n*t*-test\", as the math is easier.\n\nImagine we assess people's impressions of a training given in an\norganization. We assess attitudes toward the training on a -5 (very\nnegative) to +5 (very positive) scale (zero = neutral opinion).\n\nThe question is whether people have a positive or negative attitude\ntoward the training, on average. Let's imagine that they actually have a\npositive attitude, that in the population the mean is really 2.4 (i.e.,\n*μ* = 2.4) with a standard deviation of 2.0. This is a simulated example\n(in real life, you would have no idea what the population value is:\nthat's why you're doing research). Still, by showing you a simulated\nexample, we can see how the procedure works.\n\nWhat would the null hypothesis be, here? Well, the null hypothesis\nalways states that the effect is absent. In this case, an \"effect\" would\nbe a non-zero attitude. Thus, in this case, *H*<sub>0</sub> : *μ* = 0.\n\nLet's pull a random sample of 100 scores from that population.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\nEntão, em conclusão, a hipótese nula diz que qualquer efeito que você seja\nestudar é *zero* na população e *os resultados da sua amostra são devidos a\nchance aleatória.*\n\nEssa possibilidade paira ameaçadoramente sobre todos os achados de pesquisa baseados em dados de amostras. Como sabemos que os efeitos em que confiamos todos os dias efeitos da medicina, práticas de liderança testadas etc.) são reais e não\napenas flukes devido a erro de amostragem aleatória? Precisamos encontrar uma maneira de testar a hipótese nula e veja se podemos rejeitar essa possibilidade.\n\nTeste de significância de hipótese nula: o valor *p*\n==================================================\n\nPara testar a hipótese nula, simplesmente perguntamos:*se a hipótese nula foi\nverdade, que porcentagem de tempo eu obteria esse resultado tão grande?*\nA resposta para essa pergunta é chamada de valor *p*.\n\nHá muita confusão sobre os valores *p*, então vamos revisar:\n\n- valores *p* representam quantas vezes você pode obter um resultado tão grande quanto você fez *se o nulo fosse verdadeiro*\n-*p*-valores, portanto, representam quão fácil / difícil seria obter um\n    resultado por acaso\n- os valores *p* **não** indicam a probabilidade de o resultado ser\n    devido ao acaso; apenas a probabilidade de ver *seu resultado* se o\n    null eram verdadeiros\n- Se o valor*p* para um resultado for pequeno, seria raro obter esse valor\n    resultado por acaso (ou seja, se o nulo for verdadeiro)\n- Se o valor *p* para um resultado for grande, seria comum obter\n    esse resultado por acaso (ou seja, se o nulo for verdadeiro)\n- Conclusão: o valor *p* é uma medida de \"incompatibilidade\" entre\n    seu resultado e o nulo. Se o valor *p* for pequeno, um dos dois\n    (os dados ou o nulo) provavelmente está errado. Optamos por confiar em nossos dados\n    e rejeite o nulo.\n    \n\nPara ser claro: o valor-p é uma maneira inversa de testar a hipótese nula. Gostaríamos muito de saber a probabilidade de que a hipótese nula seja verdadeira - a probabilidade de os resultados serem devidos ao acaso - mas não podemos saber isso. Você ouvirá frequentemente o valor-p descrito dessa maneira, mas isso é muito errado.\n\nPortanto, repetindo, o valor p indica a probabilidade de obter o resultado se o nulo for verdadeiro. É essencialmente uma declaração de incompatibilidade entre seus dados e o nulo. Um pequeno valor p (normalmente, menor que 5% ou \"<. 05\") informa que os dados e nulo são altamente incompatíveis. Como você realmente observou os dados, conclui que a hipótese nula é falsa. Este é o único uso para o valor p.\n\n\nDe onde vêm os valores*p*?\n==============================\n\nDe onde vem um valor*p* ? Toda situação de dados é diferente, mas\no processo nas chamadas estatísticas \"freqüentistas\" é sempre o mesmo\n\n1. Observe os dados e examine o resultado\n2. Calcule a \"estatística de teste\" apropriada para esse resultado (por exemplo,*t*\n    teste,*z* teste,*χ*<sup>2</sup> teste,*F*teste,*q*teste etc.).\n3. Observe com que frequência você pode obter a estatística de teste observada se o\n    hipótese nula era verdadeira. Este é o valor *p*\n4. Se o valor *p* for menor que 0,05, declare o resultado \"significativo\"\n    e rejeitar a hipótese nula\n\nVamos ver isso em ação. Neste exemplo, usarei uma \"amostra única\n*t*-test \", pois a matemática é mais fácil.\n\nImagine que avaliamos as impressões das pessoas de um treinamento ministrado em um\norganização. Avaliamos atitudes em relação ao treinamento em um -5 (muito\nnegativo) a +5 (muito positivo) na escala (zero = opinião neutra).\n\nA questão é se as pessoas têm uma atitude positiva ou negativa\nem direção ao treinamento, em média. Vamos imaginar que eles realmente têm um\natitude positiva, que na população a média é realmente 2,4 (ou seja,\n*μ*= 2,4) com um desvio padrão de 2,0. Este é um exemplo simulado\n(na vida real, você não teria idéia do valor da população:\né por isso que você está pesquisando). Ainda assim, mostrando uma simulação\nPor exemplo, podemos ver como o procedimento funciona.\n\nQual seria a hipótese nula aqui? Bem, a hipótese nula\nsempre afirma que o efeito está ausente. Nesse caso, um \"efeito\" seria\numa atitude diferente de zero. Assim, neste caso,*H*<sub>0</sub>:*μ*= 0.\n\nVamos extrair uma amostra aleatória de 100 pontuações dessa população.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "nr.seed(4455)\nattitude = nr.normal(2.4, 2.0, 100)\n#`normal(mean, std, n)`",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "What are the mean and SD in our sample?\n___\nQual é a média e o desvio padrão em nossa amostra?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(np.mean(attitude))\n\nprint(np.std(attitude))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "text": "2.234095719379859\n2.0725742818363613\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "#### So, our null hypothesis is that the mean is zero\n(*H*<sub>0</sub> : *μ* = 0) but our sample result disagrees with that\n(sample mean = 2.23).\n\nDoes this *sample* gives us enough evidence to reject the null?\n\nTo answer that question, we calculate a test statistic. In this case\n(one group, sample mean), we conduct a one-group *t*-test for means. (As\nyou progress in your data science and statistics knowledge, you will\nlearn when to use different kinds of tests.)\n\nIn the *t*-test, we compare the size of the difference between our\nobserved result and the null hypothesis, divided by what you would\ntypically expect by chance (i.e., standard error):\n\n$$t=\\frac{result - null }{chance}$$\n\nSince our sample result is a sample mean ($\\\\bar{x}$), and we know the\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}}$$\n\nWe can plug in our numbers easily:\n\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}} =  \\frac{2.234-0}{\\frac{2.073}{\\sqrt{100}}} = 10.8$$\n The test assesses how much the data disagree with the null (i.e., the\neffect; top of fraction) compared to what you would typically expect by\nchance (bottom of fraction). Thus, we can literally read the result as\nsaying \"our effect was 10.8 times greater than you would typically\nexpect by chance.\" That sounds pretty good for our effect and pretty bad\nfor the null hypothesis.\n\nIt is convenient that the *t*-test works this way. However, truth be\ntold, the test statistic need not have *any* intuitive meaning. To get\nour *p*-value, the only thing we need to do is assess how rare our\nresult would be if the null hypothesis was true. Thus, it doesn't really\nmatter if we can interpret the *p*-value directly. We simply need to\nknow where *t*-test results tend to be when the null is true, and then\nwe can see how rare a score of 10.8 would be in that situation, giving\nus our *p*-value.\n\nThis is an easy question to answer. Statisticians have mapped out the\nexact behavior of each test statistic when the null hypothesis is true\n(or as we often say, \"under the null\"). We know, for example, that if\nthe null hypothesis is true, that the *t*-test will be close to zero\n(almost always within +/- 3 points of zero). So, what is our *p*-value? If\nthe null were true, how often could we get *t*-test result as big as\n10.8?\n\nUsing Python\n=======\n\nWith a bit of programming, Python will do all of this work for you:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Portanto, nossa hipótese nula é que a média é zero\n\n(*H*<sub>0</sub> : *μ* = 0) , mas o resultado da nossa amostra não concorda com isso (média da amostra = 2,23).\n\nEsta amostra nos fornece evidências suficientes para rejeitar o nulo?\n\nPara responder a essa pergunta, calculamos uma estatística de teste. Nesse caso (um grupo, média amostral), realizamos um teste t de um grupo para médias. (À medida que avança no conhecimento de ciência de dados e estatística, você aprenderá quando usar diferentes tipos de testes.)\n\nNo teste t, comparamos o tamanho da diferença entre o resultado observado e a hipótese nula, dividido pelo que você normalmente esperaria por acaso (ou seja, erro padrão):\n\n$$t=\\frac{result - null }{chance}$$\n\nComo o resultado da amostra é uma média da amostra (𝑏𝑎𝑟𝑥\n) e conhecemos o\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}}$$\n\nPodemos conectar nossos números facilmente:\n\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}} =  \\frac{2.234-0}{\\frac{2.073}{\\sqrt{100}}} = 10.8$$\n\nO teste avalia quanto os dados discordam dos nulos (ou seja, o efeito; parte superior da fração) em comparação com o que você normalmente esperaria por acaso (parte inferior da fração). Assim, podemos literalmente ler o resultado dizendo \"nosso efeito foi 10,8 vezes maior do que você normalmente esperaria por acaso\". Isso parece muito bom para o nosso efeito e muito ruim para a hipótese nula.\n\nÉ conveniente que o teste t funcione dessa maneira. No entanto, verdade seja dita, a estatística do teste não precisa ter nenhum significado intuitivo. Para obter nosso valor-p, a única coisa que precisamos fazer é avaliar quão raro seria o nosso resultado se a hipótese nula fosse verdadeira. Portanto, não importa se podemos interpretar o valor-p diretamente. Simplesmente precisamos saber onde os resultados do teste t tendem a ser quando o nulo é verdadeiro e, em seguida, podemos ver quão rara seria uma pontuação de 10,8 nessa situação, dando-nos nosso valor-p.\n\nEsta é uma pergunta fácil de responder. Os estatísticos mapearam o comportamento exato de cada estatística de teste quando a hipótese nula é verdadeira (ou, como costumamos dizer, \"abaixo do nulo\"). Sabemos, por exemplo, que se a hipótese nula for verdadeira, que o teste t será próximo de zero (quase sempre dentro de +/- 3 pontos de zero). Então, qual é o nosso valor p? Se o nulo fosse verdadeiro, com que frequência conseguiríamos o resultado do teste t tão grande quanto 10,8?\n## Usando Python\n\nCom um pouco de programação, o Python fará todo esse trabalho para você:\n\nstats . ttest_1samp ( array, 0.0 ) => teste t\n\nstats . t . cdf (0.05/2, len(array), loc=0.0, scale=scale) => intervalo de confiança",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from scipy import stats\ndef t_one_sample(samp, mu = 0.0, alpha = 0.05):\n    '''Function for two-sided one-sample t test'''\n    t_stat = stats.ttest_1samp(samp, mu)\n    scale = np.std(samp)\n    loc = np.mean(samp)\n    ci = stats.t.cdf(alpha/2, len(samp), loc=mu, scale=scale)\n    print('Results of one-sample two-sided t test')\n    print('Mean         = %4.3f' % loc)\n    print('t-Statistic  = %4.3f' % t_stat[0])\n    print('p-value      < %4.3e' % t_stat[1])\n    print('On degrees of freedom = %4d' % (len(samp) - 1))\n    print('Confidence Intervals for alpha =' + str(alpha))\n    print('Confidence Intervals =' + str(ci))\n    print('Lower =  %4.3f Upper = %4.3f' % (loc - ci, loc + ci))\n    \nt_one_sample(attitude)    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "text": "Results of one-sample two-sided t test\nMean         = 2.234\nt-Statistic  = 10.725\np-value      < 2.881e-18\nOn degrees of freedom =   99\nConfidence Intervals for alpha =0.05\nConfidence Intervals =0.504800026276705\nLower =  1.729 Upper = 2.739\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "stats.ttest_1samp(attitude, 0.0)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Ttest_1sampResult(statistic=10.725295559043051, pvalue=2.8805721838958042e-18)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "The key information is from this function is:\n`t statistic = 10.7, df = 99, p-value < 2.9e-18`. Notice that the *p*-value is displayed in scientific notation. `2.9e-18` is scientific notation:\n2.9 x 10<sup>-18</sup> and means the same as 0.0000000000000000029. This\nis clearly less than .05 so we can reject the null hypothesis and\nconclude that the positive attitude observed among our participants was\nnot a statistical fluke but likely a real trend in the population.\n___\nA informação principal desta função é: t estatística = 10.7, df = 99, valor de p <2.9e-18. Observe que o valor p é exibido em notação científica. 2.9e-18 é uma notação científica: 2.9 x 10-18 e significa o mesmo que 0.0000000000000000029. Isso é claramente menor que 0,05, para que possamos rejeitar a hipótese nula e concluir que a atitude positiva observada entre nossos participantes não foi um acaso estatístico, mas provavelmente uma tendência real na população.\n___\n\n### For Illustration Purposes\n\nHow did Statsmodels compute that *p*-value? I will illustrate.\n\nI start with a plot of all the *t*-test results (for sample size of 100)\nyou would expect **if the null hypothesis was true.** We know this,\nthanks to mathematicians.\n___\n### Para fins de ilustração¶\n\nComo o Statsmodels calculou esse valor-p? Vou ilustrar.\n\nComeço com um gráfico de todos os resultados do teste t (para o tamanho da amostra de 100) que você esperaria se a hipótese nula fosse verdadeira. Sabemos disso, graças aos matemáticos.\n___\n![](img/unnamed-chunk-8-1.png)\n\nThe bell curve above illustrates all the possible *t*-test results one\nwould expect when the null is true and their respective probabilities.\nWe see here that most results are within about +/- 3 points from zero.\nWhere is our result? Let's add it to the plot.\n___\nA curva de sino acima ilustra todos os resultados possíveis do teste * t *\nseria de esperar quando o nulo for verdadeiro e suas respectivas probabilidades.\nVemos aqui que a maioria dos resultados está dentro de +/- 3 pontos a partir do zero.\nOnde está o nosso resultado? Vamos adicioná-lo à trama.\n___\n\n![](img/unnamed-chunk-9-1.png)\n\nAs we see, our result is out among values that are very, very rare under\nthe null hypothesis. It appears that our data disagree the null\nhypothesis. When the null is true, we should be getting *t*-test results\ndown in the center of the bell curve (approximately ± 3), but we didn't.\nWe were up at 12.8.\n___\nComo vemos, nosso resultado está entre valores muito, muito raros sob\na hipótese nula. Parece que nossos dados discordam do valor nulo\nhipótese. Quando o nulo for verdadeiro, deveríamos obter resultados * t * -test\nno centro da curva do sino (aproximadamente ± 3), mas não o fizemos.\nNós chegamos às 12.8.\n___\n\nTo find the *p*-value, we simply ask what percentage of our *t*-curve is\nout that far. In other words, what proportion of the bell curve extends\nout beyond the red line? What is the area \"in the upper tail\"?\n___\nPara encontrar o valor * p *, simplesmente perguntamos qual a porcentagem de nossa curva * t *\ntão longe. Em outras palavras, qual a proporção da curva de sino se estende\nalém da linha vermelha? Qual é a área \"na cauda superior\"?\n___\nWe can compute the p-value as $1 - cdf$, for the t-statistic, where $cdf$ is the cumulative density function. The statsmodels `t.cdf()` function computes the cdf given the t-statistic and the degrees of freedom; $n − 1 = 100 − 1 = 99$:\n\n___\nPodemos calcular o valor-p como $ 1 - cdf $, para a estatística t, onde $ cdf $ é a função de densidade cumulativa. A função statsmodels `t.cdf ()` calcula o cdf, dada a estatística t e os graus de liberdade; $n − 1 = 100 − 1 = 99$:\n___",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from scipy.stats import t\n1 - t.cdf(10.8, df = 99, loc=0, scale=1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "This result is saying there is \"zero\" probability of getting a result this big if\nthe null were true; i.e., *p* = 0. In reality, *p* values are never zero\nbut can get infinitely small. In this case the a tiny number is rounded to 0.\n___\nEste resultado está dizendo que há probabilidade \"zero\" de obter um resultado tão grande se o nulo for verdadeiro; isto é, p = 0. Na realidade, os valores de p nunca são zero, mas podem ficar infinitamente pequenos. Nesse caso, o número minúsculo é arredondado para 0.\n___\n\nThis is called a one-tailed *p*-value. We actually, however, need to\ndouble it. The reason we need to double it is that our null hypothesis\nwas that *μ* = 0. That is, the null is false if our result is\nsignificantly *larger* than zero (a positive attitude) or significantly\n*smaller* than zero (a negative attitude). This is consistent with how\nwe asked our question: \"do people have positive or negative attitudes?\"\nIn other words, we did not test a directional prediction; we would be\ninterested in \"finding\" something regardless of the direction of the\neffect. Since the *p*-value is the probability of getting an effect\n\"this large\" and we do not care about the direction, it actually exists\non both sides of the distribution (a negative attitude would have given\nus a negative *t*-score):\n___\n\nIsso é chamado de valor p unicaudal. Na verdade, precisamos dobrá-lo. A razão pela qual precisamos dobrar é que nossa hipótese nula foi a de μ = 0. Ou seja, o nulo será falso se nosso resultado for significativamente maior que zero (uma atitude positiva) ou significativamente menor que zero (uma atitude negativa). Isso é consistente com a forma como fizemos nossa pergunta: \"as pessoas têm atitudes positivas ou negativas?\" Em outras palavras, não testamos uma previsão direcional; estaríamos interessados ​​em \"encontrar\" algo, independentemente da direção do efeito. Como o valor p é a probabilidade de obter um efeito \"desse tamanho\" e não nos importamos com a direção, ele realmente existe nos dois lados da distribuição (uma atitude negativa nos daria um escore t negativo):\n___\n\n![](img/unnamed-chunk-11-1.png)\n\nThus, we have to double our *p*-value. This is standard practice any\ntime you would be willing to declare the result significant **regardless\nof the direction**. We call this a *two-tailed p-value*.\n___\nPortanto, temos que dobrar nosso valor-p. Essa é uma prática padrão sempre que você estiver disposto a declarar o resultado significativo, independentemente da direção. Chamamos isso de um valor p bicaudal.\n___\nIf this explanation is confusing, you can also understand it a slightly\ndifferent way: by testing *H*<sub>0</sub> : *μ* = 0, you are really\nasking whether *μ* &lt; 0 or whether *μ* &gt; 0. You are essentially\nasking two separate questions of the data. You need to double your\n*p*-value.\n\n___\n\nSe essa explicação é confusa, você também pode entendê-la de uma maneira um pouco diferente: ao testar H0: μ = 0, você está realmente perguntando se μ <0 ou se μ> 0. Você está essencialmente fazendo duas perguntas separadas dos dados. Você precisa dobrar seu valor-p.\n___\nThis is almost always what you want. We almost always want to be able to\ndeclare a result significant if the effect is large, regardless of\nwhether the direction of the result matches our intuition or not. For\nexample, if an intervention to increase productivity backfires and\ndecreases productivity, we want to know that just as much as we want to\nknow if it works.\n___\n\nIsso é quase sempre o que você deseja. Quase sempre queremos declarar um resultado significativo se o efeito for grande, independentemente de a direção do resultado corresponder à nossa intuição ou não. Por exemplo, se uma intervenção para aumentar a produtividade sai pela culatra e diminui a produtividade, queremos saber disso tanto quanto queremos saber se funciona.\n___\nThus, we almost always double the *p*-value for this reason. It is true\nthat it makes it a little harder to get a significant result (less than\n.05), but we can extract more meaning from the result. It's worth it.\n\nNote: our doubled *p*-value here is still essentially zero:\n___\n\nAssim, quase sempre dobramos o valor de p por esse motivo. É verdade que torna um pouco mais difícil obter um resultado significativo (menor que 0,05), mas podemos extrair mais significado do resultado. Vale a pena.\n\nNota: nosso valor de p duplicado aqui ainda é essencialmente zero:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "2.0 * (1 - t.cdf(10.8, df = 99, loc=0, scale=1))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}