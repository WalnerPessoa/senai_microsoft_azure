{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat_minor": 2,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Module 2, Lab 2: *p*-values\n===========================\n\nIn this lab, we will explore how the basics of null hypothesis\nsignificance testing work. Although you may have examined this in a\nprevious course, we will review the concepts of *p*-values and tests of\nstatistical significance with an emphasis on their application in\nresearch.\n\n___\nM√≥dulo 2, Laborat√≥rio 2: valores p\n===========================\n\nNeste laborat√≥rio, exploraremos como os conceitos b√°sicos trabalho de teste de signific√¢ncia de hip√≥tese nula. Embora voc√™ possa ter examinado isso em um No curso anterior, revisaremos os conceitos de valores p e testes de signific√¢ncia estat√≠stica com √™nfase em sua aplica√ß√£o em pesquisa.\n___\n\n\nThe Null Hypothesis\n===================\n\nFirst, we briefly review what the null hypothesis is. Recall from the\nprevious lab that the results that come from samples are only mere\nestimates of the population. Because they are estimates, the statistics\nthey produce will differ somewhat from their population counterparts.\nFor example, the correlation between engagement in a sample may be *r* =\n.2 even when the correlation between those same variables in the\npopulation is something smaller, such as .10 or even 0. This can cause\napparent relationships and effects to appear in *samples* when none in\nfact exists in the population. This idea--that the effect/association is\n*zero* in the population--is called the null hypothesis. By implication,\nany effect/association seen in the *sample* must be entirely due to the\nrandom chance of \"sampling error.\" In other words, the null hypothesis\nclaims that the sample result is a random fluke.\n___\nA hip√≥tese nula¬∂\n===================\n\nPrimeiro, revisamos brevemente qual √© a hip√≥tese nula. Lembre-se do laborat√≥rio anterior que os resultados provenientes de amostras s√£o apenas meros estimativas da popula√ß√£o. Por serem estimativas, as estat√≠sticas eles produzem diferem um pouco de suas contrapartes populacionais. Por exemplo, a correla√ß√£o entre o envolvimento em uma amostra pode ser r = .2 mesmo quando a correla√ß√£o entre essas mesmas vari√°veis ‚Äã‚Äãno popula√ß√£o √© algo menor, como 0,10 ou at√© 0. Isso pode causar aparentes rela√ß√µes e efeitos a aparecer em amostras quando nenhuma fato existe na popula√ß√£o. Esta ideia - que o efeito / associa√ß√£o √© zero na popula√ß√£o - √© chamada de hip√≥tese nula. Por implica√ß√£o, qualquer efeito / associa√ß√£o observado na amostra deve ser inteiramente devido √† chance aleat√≥ria de \"erro de amostragem\". Em outras palavras, a hip√≥tese nula afirma que o resultado da amostra √© um acaso aleat√≥rio.\n___\nLet's explore an application of this. Imagine we want to compare males\nand females in terms of their interest in a given product. Imagine, for\na moment, that *the two groups have identical interest* (in the\npopulation)...that is, there is no difference between the groups.\nNevertheless, if we take a sample of males and a sample of females, the\nerror in our estimations will cause a difference to appear.\n\nImagine that *both* males and females had an interest level averaging at\n5, with a standard deviation of 3.\n\n___\nVamos explorar uma aplica√ß√£o disso. Imagine que queremos comparar homens e mulheres em termos de interesse em um determinado produto. Imagine, por por um momento, que os dois grupos t√™m interesse id√™ntico (no popula√ß√£o) ... ou seja, n√£o h√° diferen√ßa entre os grupos. No entanto, se coletarmos uma amostra de homens e uma amostra de mulheres, o um erro nas nossas estimativas far√° com que apare√ßa uma diferen√ßa.\n\nImagine que ambos homens e mulheres tiveram um n√≠vel de interesse m√©dio de 5, com um desvio padr√£o de 3\n___",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# set seed to make random number generation reproducible\nimport numpy as np\nimport numpy.random as nr\nnr.seed(51120122)\n\n#collect a sample of 100 males\nmales = nr.normal(5, 3, 100)\n\n#collect a sample of 100 females\nfemales = nr.normal(5, 3, 100)\n\nprint(np.mean(males))\nprint(np.mean(females))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "text": "5.171234200421537\n5.898998940622083\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "We see here that our two groups have different sample results. Let's see\nhow large the difference is:\n___\nVemos aqui que nossos dois grupos t√™m resultados de amostra diferentes. Vamos ver\nqu√£o grande √© a diferen√ßa:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.mean(males)-np.mean(females)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "-0.7277647402005458"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "We see here that the females are almost 3/4 of a point higher than the\nmales. If you saw this data in an organization where you were working,\nyou might be tempted to think you'd discovered a female preference for\nyour product. However, in fact, we *know* in this case that this is\nnonsense as we *know* (because we wrote the Python code simulating this data)\nthat *both* groups were random samples from a population with a mean of\n5 and a standard deviation of 3. If their means are both 5.0 (exactly)\nin the population, why did the females score higher in the samples? It's\nsimple: sampling error. That is, the difference is **entirely** due to\nrandom error in the samples, not any real difference in the population.\nWe have discovered a fluke in some sample data, nothing more.\n___\nVemos aqui que as f√™meas s√£o quase 3/4 de um ponto mais alto que o machos. Se voc√™ viu esses dados em uma organiza√ß√£o em que estava trabalhando, voc√™ pode ficar tentado a pensar que descobriu uma prefer√™ncia feminina por seu produto. No entanto, de fato, sabemos neste caso que isso √© absurdo como conhecemos (porque escrevemos o c√≥digo Python simulando esses dados) que ambos grupos eram amostras aleat√≥rias de uma popula√ß√£o com m√©dia de 5 e um desvio padr√£o de 3. Se suas m√©dias forem ambas 5,0 (exatamente) na popula√ß√£o, por que as f√™meas pontuaram mais nas amostras? Est√° simples: erro de amostragem. Ou seja, a diferen√ßa √© inteiramente devido a erro aleat√≥rio nas amostras, nenhuma diferen√ßa real na popula√ß√£o. Descobrimos um acaso em alguns dados de amostra, nada mais.\n___\nThis is a case of the \"null hypothesis.\" In this case, the means are\n*equal* in the population. We write the null hypothesis as\n*H*<sub>0</sub> and it is always a statement that the size of the effect\nin the population is zero. In this case, we are testing the difference\nbetween the averages ($\\mu ' s$), stating that the *difference between\nthem is zero*:\n\n$$H_0 :\\ \\mu_{male} - \\mu_{female} = 0$$\n\nHowever, to reiterate what we saw above, *when we looked at our\nsamples,* we saw there was a difference:\n___\n\n\nEste √© um caso da \"hip√≥tese nula\". Nesse caso, as m√©dias s√£o\n*iguais* na popula√ß√£o. Escrevemos a hip√≥tese nula como\n*H* <sub> 0 </sub> e √© sempre uma declara√ß√£o de que o tamanho do efeito\nna popula√ß√£o √© zero. Nesse caso, estamos testando a diferen√ßa\nentre as m√©dias ($\\mu ' s$), declarando que a * diferen√ßa entre\neles √© zero *:\n\n$$H_0 :\\ \\mu_{male} - \\mu_{female} = 0$$\n\nNo entanto, para reiterar o que vimos acima, *quando analisamos nossa\namostras* vimos que havia uma diferen√ßa:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.mean(males)-np.mean(females)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "-0.7277647402005458"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "So, in conclusion, the null hypothesis says that whatever effect you are\nstudying is *zero* in the population and *your sample results are due to\nrandom chance.*\n\nThis possibility looms ominously over every research finding based on\nsamples data. How do we know that the effects we trust every day (the\neffect of medicine, tested leadership practices, etc.) are real and not\njust flukes due to random sampling error? We need to find a way to test\nthe null hypothesis and see if we can reject this possibility.\n\nNull Hypothesis Significance Test: The *p*-Value\n================================================\n\nTo test the null hypothesis, we simply ask: *if the null hypothesis were\ntrue, what percentage of the time would I get this result this large?*\nThe answer to that question is called a *p*-value.\n\nThere is a lot of confusion about *p*-values, so let's review:\n\n-   *p*-values represent how often you could get a result as big as you\n    did *if the null were true*\n-   *p*-values therefore represent how easy/hard it would be to get a\n    result by chance\n-   *p*-values do **not** tell you the probability that the result is\n    due to chance; only the probability of seeing *your result* if the\n    null were true\n-   If the *p*-value for a result is small, it would be rare to get that\n    result by chance (i.e., if the null were true)\n-   If the *p*-value for a result is large, it would be common to get\n    that result by chance (i.e., if the null were true)\n-   Conclusion: the *p*-value is a measure of \"incompatibility\" between\n    your result and the null. If the *p*-value is small, one of the two\n    (the data, or the null) is likely wrong. We opt to trust our data\n    and reject the null.\n\nTo be clear: the *p*-value is a backwards way of testing the null\nhypothesis. We would love to know the *probability* that the null\nhypothesis is true--the probability that the results *are* due to\nchance--but we cannot know that. You will often hear the *p*-value\ndescribed this way, but that is **very wrong**.\n\nSo, to repeat, the *p-value states the probability of getting **your\nresult** if the null is true*. It is essentially a statement of\nincompatibility between your data and the null. A small *p*-value\n(typically, less than 5% or \"&lt; . 05\") tells you that the data and\nnull are highly incompatible. Since you did in fact observe the data,\nyou conclude the null hypothesis is false. This is the only use for the\n*p*-value.\n\nWhere do *p*-Values Come From?\n==============================\n\nWhere does a *p*-value come from? Every data situation is different, but\nthe process in so-called \"frequentist\" statistics is always the same\n\n1.  Observe data and examine result\n2.  Compute the appropriate \"test statistic\" for that result (e.g., *t*\n    test, *z* test, *œá*<sup>2</sup> test, *F* test, *q* test etc.).\n3.  Observe how often you could get the observed test statistic if the\n    null hypothesis was true. This is the *p*-value\n4.  If the *p*-value is less than .05, declare the result \"significant\"\n    and reject the null hypothesis\n\nLet's see this in action. For this example, I will use a \"one-sample\n*t*-test\", as the math is easier.\n\nImagine we assess people's impressions of a training given in an\norganization. We assess attitudes toward the training on a -5 (very\nnegative) to +5 (very positive) scale (zero = neutral opinion).\n\nThe question is whether people have a positive or negative attitude\ntoward the training, on average. Let's imagine that they actually have a\npositive attitude, that in the population the mean is really 2.4 (i.e.,\n*Œº*‚ÄÑ=‚ÄÑ2.4) with a standard deviation of 2.0. This is a simulated example\n(in real life, you would have no idea what the population value is:\nthat's why you're doing research). Still, by showing you a simulated\nexample, we can see how the procedure works.\n\nWhat would the null hypothesis be, here? Well, the null hypothesis\nalways states that the effect is absent. In this case, an \"effect\" would\nbe a non-zero attitude. Thus, in this case, *H*<sub>0</sub>‚ÄÑ:‚ÄÑ*Œº*‚ÄÑ=‚ÄÑ0.\n\nLet's pull a random sample of 100 scores from that population.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "---\nEnt√£o, em conclus√£o, a hip√≥tese nula diz que qualquer efeito que voc√™ seja\nestudar √© *zero* na popula√ß√£o e *os resultados da sua amostra s√£o devidos a\nchance aleat√≥ria.*\n\nEssa possibilidade paira amea√ßadoramente sobre todos os achados de pesquisa baseados em dados de amostras. Como sabemos que os efeitos em que confiamos todos os dias efeitos da medicina, pr√°ticas de lideran√ßa testadas etc.) s√£o reais e n√£o\napenas flukes devido a erro de amostragem aleat√≥ria? Precisamos encontrar uma maneira de testar a hip√≥tese nula e veja se podemos rejeitar essa possibilidade.\n\nTeste de signific√¢ncia de hip√≥tese nula: o valor *p*\n==================================================\n\nPara testar a hip√≥tese nula, simplesmente perguntamos:*se a hip√≥tese nula foi\nverdade, que porcentagem de tempo eu obteria esse resultado t√£o grande?*\nA resposta para essa pergunta √© chamada de valor *p*.\n\nH√° muita confus√£o sobre os valores *p*, ent√£o vamos revisar:\n\n- valores *p* representam quantas vezes voc√™ pode obter um resultado t√£o grande quanto voc√™ fez *se o nulo fosse verdadeiro*\n-*p*-valores, portanto, representam qu√£o f√°cil / dif√≠cil seria obter um\n    resultado por acaso\n- os valores *p* **n√£o** indicam a probabilidade de o resultado ser\n    devido ao acaso; apenas a probabilidade de ver *seu resultado* se o\n    null eram verdadeiros\n- Se o valor*p* para um resultado for pequeno, seria raro obter esse valor\n    resultado por acaso (ou seja, se o nulo for verdadeiro)\n- Se o valor *p* para um resultado for grande, seria comum obter\n    esse resultado por acaso (ou seja, se o nulo for verdadeiro)\n- Conclus√£o: o valor *p* √© uma medida de \"incompatibilidade\" entre\n    seu resultado e o nulo. Se o valor *p* for pequeno, um dos dois\n    (os dados ou o nulo) provavelmente est√° errado. Optamos por confiar em nossos dados\n    e rejeite o nulo.\n    \n\nPara ser claro: o valor-p √© uma maneira inversa de testar a hip√≥tese nula. Gostar√≠amos muito de saber a probabilidade de que a hip√≥tese nula seja verdadeira - a probabilidade de os resultados serem devidos ao acaso - mas n√£o podemos saber isso. Voc√™ ouvir√° frequentemente o valor-p descrito dessa maneira, mas isso √© muito errado.\n\nPortanto, repetindo, o valor p indica a probabilidade de obter o resultado se o nulo for verdadeiro. √â essencialmente uma declara√ß√£o de incompatibilidade entre seus dados e o nulo. Um pequeno valor p (normalmente, menor que 5% ou \"<. 05\") informa que os dados e nulo s√£o altamente incompat√≠veis. Como voc√™ realmente observou os dados, conclui que a hip√≥tese nula √© falsa. Este √© o √∫nico uso para o valor p.\n\n\nDe onde v√™m os valores*p*?\n==============================\n\nDe onde vem um valor*p* ? Toda situa√ß√£o de dados √© diferente, mas\no processo nas chamadas estat√≠sticas \"freq√ºentistas\" √© sempre o mesmo\n\n1. Observe os dados e examine o resultado\n2. Calcule a \"estat√≠stica de teste\" apropriada para esse resultado (por exemplo,*t*\n    teste,*z* teste,*œá*<sup>2</sup> teste,*F*teste,*q*teste etc.).\n3. Observe com que frequ√™ncia voc√™ pode obter a estat√≠stica de teste observada se o\n    hip√≥tese nula era verdadeira. Este √© o valor *p*\n4. Se o valor *p* for menor que 0,05, declare o resultado \"significativo\"\n    e rejeitar a hip√≥tese nula\n\nVamos ver isso em a√ß√£o. Neste exemplo, usarei uma \"amostra √∫nica\n*t*-test \", pois a matem√°tica √© mais f√°cil.\n\nImagine que avaliamos as impress√µes das pessoas de um treinamento ministrado em um\norganiza√ß√£o. Avaliamos atitudes em rela√ß√£o ao treinamento em um -5 (muito\nnegativo) a +5 (muito positivo) na escala (zero = opini√£o neutra).\n\nA quest√£o √© se as pessoas t√™m uma atitude positiva ou negativa\nem dire√ß√£o ao treinamento, em m√©dia. Vamos imaginar que eles realmente t√™m um\natitude positiva, que na popula√ß√£o a m√©dia √© realmente 2,4 (ou seja,\n*Œº*= 2,4) com um desvio padr√£o de 2,0. Este √© um exemplo simulado\n(na vida real, voc√™ n√£o teria id√©ia do valor da popula√ß√£o:\n√© por isso que voc√™ est√° pesquisando). Ainda assim, mostrando uma simula√ß√£o\nPor exemplo, podemos ver como o procedimento funciona.\n\nQual seria a hip√≥tese nula aqui? Bem, a hip√≥tese nula\nsempre afirma que o efeito est√° ausente. Nesse caso, um \"efeito\" seria\numa atitude diferente de zero. Assim, neste caso,*H*<sub>0</sub>:*Œº*= 0.\n\nVamos extrair uma amostra aleat√≥ria de 100 pontua√ß√µes dessa popula√ß√£o.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "nr.seed(4455)\nattitude = nr.normal(2.4, 2.0, 100)\n#`normal(mean, std, n)`",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "What are the mean and SD in our sample?\n___\nQual √© a m√©dia e o desvio padr√£o em nossa amostra?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(np.mean(attitude))\n\nprint(np.std(attitude))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "text": "2.234095719379859\n2.0725742818363613\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "#### So, our null hypothesis is that the mean is zero\n(*H*<sub>0</sub>‚ÄÑ:‚ÄÑ*Œº*‚ÄÑ=‚ÄÑ0) but our sample result disagrees with that\n(sample mean = 2.23).\n\nDoes this *sample* gives us enough evidence to reject the null?\n\nTo answer that question, we calculate a test statistic. In this case\n(one group, sample mean), we conduct a one-group *t*-test for means. (As\nyou progress in your data science and statistics knowledge, you will\nlearn when to use different kinds of tests.)\n\nIn the *t*-test, we compare the size of the difference between our\nobserved result and the null hypothesis, divided by what you would\ntypically expect by chance (i.e., standard error):\n\n$$t=\\frac{result - null }{chance}$$\n\nSince our sample result is a sample mean ($\\\\bar{x}$), and we know the\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}}$$\n\nWe can plug in our numbers easily:\n\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}} =  \\frac{2.234-0}{\\frac{2.073}{\\sqrt{100}}} = 10.8$$\n The test assesses how much the data disagree with the null (i.e., the\neffect; top of fraction) compared to what you would typically expect by\nchance (bottom of fraction). Thus, we can literally read the result as\nsaying \"our effect was 10.8 times greater than you would typically\nexpect by chance.\" That sounds pretty good for our effect and pretty bad\nfor the null hypothesis.\n\nIt is convenient that the *t*-test works this way. However, truth be\ntold, the test statistic need not have *any* intuitive meaning. To get\nour *p*-value, the only thing we need to do is assess how rare our\nresult would be if the null hypothesis was true. Thus, it doesn't really\nmatter if we can interpret the *p*-value directly. We simply need to\nknow where *t*-test results tend to be when the null is true, and then\nwe can see how rare a score of 10.8 would be in that situation, giving\nus our *p*-value.\n\nThis is an easy question to answer. Statisticians have mapped out the\nexact behavior of each test statistic when the null hypothesis is true\n(or as we often say, \"under the null\"). We know, for example, that if\nthe null hypothesis is true, that the *t*-test will be close to zero\n(almost always within +/- 3 points of zero). So, what is our *p*-value? If\nthe null were true, how often could we get *t*-test result as big as\n10.8?\n\nUsing Python\n=======\n\nWith a bit of programming, Python will do all of this work for you:",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Portanto, nossa hip√≥tese nula √© que a m√©dia √© zero\n\n(*H*<sub>0</sub>‚ÄÑ:‚ÄÑ*Œº*‚ÄÑ=‚ÄÑ0)‚ÄÑ, mas o resultado da nossa amostra n√£o concorda com isso (m√©dia da amostra = 2,23).\n\nEsta amostra nos fornece evid√™ncias suficientes para rejeitar o nulo?\n\nPara responder a essa pergunta, calculamos uma estat√≠stica de teste. Nesse caso (um grupo, m√©dia amostral), realizamos um teste t de um grupo para m√©dias. (√Ä medida que avan√ßa no conhecimento de ci√™ncia de dados e estat√≠stica, voc√™ aprender√° quando usar diferentes tipos de testes.)\n\nNo teste t, comparamos o tamanho da diferen√ßa entre o resultado observado e a hip√≥tese nula, dividido pelo que voc√™ normalmente esperaria por acaso (ou seja, erro padr√£o):\n\n$$t=\\frac{result - null }{chance}$$\n\nComo o resultado da amostra √© uma m√©dia da amostra (ùëèùëéùëüùë•\n) e conhecemos o\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}}$$\n\nPodemos conectar nossos n√∫meros facilmente:\n\n$$t = \\frac{\\bar{x}-H_0}{\\frac{SD}{\\sqrt{n}}} =  \\frac{2.234-0}{\\frac{2.073}{\\sqrt{100}}} = 10.8$$\n\nO teste avalia quanto os dados discordam dos nulos (ou seja, o efeito; parte superior da fra√ß√£o) em compara√ß√£o com o que voc√™ normalmente esperaria por acaso (parte inferior da fra√ß√£o). Assim, podemos literalmente ler o resultado dizendo \"nosso efeito foi 10,8 vezes maior do que voc√™ normalmente esperaria por acaso\". Isso parece muito bom para o nosso efeito e muito ruim para a hip√≥tese nula.\n\n√â conveniente que o teste t funcione dessa maneira. No entanto, verdade seja dita, a estat√≠stica do teste n√£o precisa ter nenhum significado intuitivo. Para obter nosso valor-p, a √∫nica coisa que precisamos fazer √© avaliar qu√£o raro seria o nosso resultado se a hip√≥tese nula fosse verdadeira. Portanto, n√£o importa se podemos interpretar o valor-p diretamente. Simplesmente precisamos saber onde os resultados do teste t tendem a ser quando o nulo √© verdadeiro e, em seguida, podemos ver qu√£o rara seria uma pontua√ß√£o de 10,8 nessa situa√ß√£o, dando-nos nosso valor-p.\n\nEsta √© uma pergunta f√°cil de responder. Os estat√≠sticos mapearam o comportamento exato de cada estat√≠stica de teste quando a hip√≥tese nula √© verdadeira (ou, como costumamos dizer, \"abaixo do nulo\"). Sabemos, por exemplo, que se a hip√≥tese nula for verdadeira, que o teste t ser√° pr√≥ximo de zero (quase sempre dentro de +/- 3 pontos de zero). Ent√£o, qual √© o nosso valor p? Se o nulo fosse verdadeiro, com que frequ√™ncia conseguir√≠amos o resultado do teste t t√£o grande quanto 10,8?\n## Usando Python\n\nCom um pouco de programa√ß√£o, o Python far√° todo esse trabalho para voc√™:\n\nstats . ttest_1samp ( array, 0.0 ) => teste t\n\nstats . t . cdf (0.05/2, len(array), loc=0.0, scale=scale) => intervalo de confian√ßa",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from scipy import stats\ndef t_one_sample(samp, mu = 0.0, alpha = 0.05):\n    '''Function for two-sided one-sample t test'''\n    t_stat = stats.ttest_1samp(samp, mu)\n    scale = np.std(samp)\n    loc = np.mean(samp)\n    ci = stats.t.cdf(alpha/2, len(samp), loc=mu, scale=scale)\n    print('Results of one-sample two-sided t test')\n    print('Mean         = %4.3f' % loc)\n    print('t-Statistic  = %4.3f' % t_stat[0])\n    print('p-value      < %4.3e' % t_stat[1])\n    print('On degrees of freedom = %4d' % (len(samp) - 1))\n    print('Confidence Intervals for alpha =' + str(alpha))\n    print('Confidence Intervals =' + str(ci))\n    print('Lower =  %4.3f Upper = %4.3f' % (loc - ci, loc + ci))\n    \nt_one_sample(attitude)    ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "text": "Results of one-sample two-sided t test\nMean         = 2.234\nt-Statistic  = 10.725\np-value      < 2.881e-18\nOn degrees of freedom =   99\nConfidence Intervals for alpha =0.05\nConfidence Intervals =0.504800026276705\nLower =  1.729 Upper = 2.739\n",
          "name": "stdout",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "stats.ttest_1samp(attitude, 0.0)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Ttest_1sampResult(statistic=10.725295559043051, pvalue=2.8805721838958042e-18)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "The key information is from this function is:\n`t statistic = 10.7, df = 99, p-value < 2.9e-18`. Notice that the *p*-value is displayed in scientific notation. `2.9e-18` is scientific notation:\n2.9 x 10<sup>-18</sup> and means the same as 0.0000000000000000029. This\nis clearly less than .05 so we can reject the null hypothesis and\nconclude that the positive attitude observed among our participants was\nnot a statistical fluke but likely a real trend in the population.\n___\nA informa√ß√£o principal desta fun√ß√£o √©: t estat√≠stica = 10.7, df = 99, valor de p <2.9e-18. Observe que o valor p √© exibido em nota√ß√£o cient√≠fica. 2.9e-18 √© uma nota√ß√£o cient√≠fica: 2.9 x 10-18 e significa o mesmo que 0.0000000000000000029. Isso √© claramente menor que 0,05, para que possamos rejeitar a hip√≥tese nula e concluir que a atitude positiva observada entre nossos participantes n√£o foi um acaso estat√≠stico, mas provavelmente uma tend√™ncia real na popula√ß√£o.\n___\n\n### For Illustration Purposes\n\nHow did Statsmodels compute that *p*-value? I will illustrate.\n\nI start with a plot of all the *t*-test results (for sample size of 100)\nyou would expect **if the null hypothesis was true.** We know this,\nthanks to mathematicians.\n___\n### Para fins de ilustra√ß√£o¬∂\n\nComo o Statsmodels calculou esse valor-p? Vou ilustrar.\n\nCome√ßo com um gr√°fico de todos os resultados do teste t (para o tamanho da amostra de 100) que voc√™ esperaria se a hip√≥tese nula fosse verdadeira. Sabemos disso, gra√ßas aos matem√°ticos.\n___\n![](img/unnamed-chunk-8-1.png)\n\nThe bell curve above illustrates all the possible *t*-test results one\nwould expect when the null is true and their respective probabilities.\nWe see here that most results are within about +/- 3 points from zero.\nWhere is our result? Let's add it to the plot.\n___\nA curva de sino acima ilustra todos os resultados poss√≠veis do teste * t *\nseria de esperar quando o nulo for verdadeiro e suas respectivas probabilidades.\nVemos aqui que a maioria dos resultados est√° dentro de +/- 3 pontos a partir do zero.\nOnde est√° o nosso resultado? Vamos adicion√°-lo √† trama.\n___\n\n![](img/unnamed-chunk-9-1.png)\n\nAs we see, our result is out among values that are very, very rare under\nthe null hypothesis. It appears that our data disagree the null\nhypothesis. When the null is true, we should be getting *t*-test results\ndown in the center of the bell curve (approximately ¬± 3), but we didn't.\nWe were up at 12.8.\n___\nComo vemos, nosso resultado est√° entre valores muito, muito raros sob\na hip√≥tese nula. Parece que nossos dados discordam do valor nulo\nhip√≥tese. Quando o nulo for verdadeiro, dever√≠amos obter resultados * t * -test\nno centro da curva do sino (aproximadamente ¬± 3), mas n√£o o fizemos.\nN√≥s chegamos √†s 12.8.\n___\n\nTo find the *p*-value, we simply ask what percentage of our *t*-curve is\nout that far. In other words, what proportion of the bell curve extends\nout beyond the red line? What is the area \"in the upper tail\"?\n___\nPara encontrar o valor * p *, simplesmente perguntamos qual a porcentagem de nossa curva * t *\nt√£o longe. Em outras palavras, qual a propor√ß√£o da curva de sino se estende\nal√©m da linha vermelha? Qual √© a √°rea \"na cauda superior\"?\n___\nWe can compute the p-value as $1 - cdf$, for the t-statistic, where $cdf$ is the cumulative density function. The statsmodels `t.cdf()` function computes the cdf given the t-statistic and the degrees of freedom; $n‚ÄÖ‚àí‚ÄÖ1‚ÄÑ=‚ÄÑ100‚ÄÖ‚àí‚ÄÖ1‚ÄÑ=‚ÄÑ99$:\n\n___\nPodemos calcular o valor-p como $ 1 - cdf $, para a estat√≠stica t, onde $ cdf $ √© a fun√ß√£o de densidade cumulativa. A fun√ß√£o statsmodels `t.cdf ()` calcula o cdf, dada a estat√≠stica t e os graus de liberdade; $n‚ÄÖ‚àí‚ÄÖ1‚ÄÑ=‚ÄÑ100‚ÄÖ‚àí‚ÄÖ1‚ÄÑ=‚ÄÑ99$:\n___",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from scipy.stats import t\n1 - t.cdf(10.8, df = 99, loc=0, scale=1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "This result is saying there is \"zero\" probability of getting a result this big if\nthe null were true; i.e., *p* = 0. In reality, *p* values are never zero\nbut can get infinitely small. In this case the a tiny number is rounded to 0.\n___\nEste resultado est√° dizendo que h√° probabilidade \"zero\" de obter um resultado t√£o grande se o nulo for verdadeiro; isto √©, p = 0. Na realidade, os valores de p nunca s√£o zero, mas podem ficar infinitamente pequenos. Nesse caso, o n√∫mero min√∫sculo √© arredondado para 0.\n___\n\nThis is called a one-tailed *p*-value. We actually, however, need to\ndouble it. The reason we need to double it is that our null hypothesis\nwas that *Œº*‚ÄÑ=‚ÄÑ0. That is, the null is false if our result is\nsignificantly *larger* than zero (a positive attitude) or significantly\n*smaller* than zero (a negative attitude). This is consistent with how\nwe asked our question: \"do people have positive or negative attitudes?\"\nIn other words, we did not test a directional prediction; we would be\ninterested in \"finding\" something regardless of the direction of the\neffect. Since the *p*-value is the probability of getting an effect\n\"this large\" and we do not care about the direction, it actually exists\non both sides of the distribution (a negative attitude would have given\nus a negative *t*-score):\n___\n\nIsso √© chamado de valor p unicaudal. Na verdade, precisamos dobr√°-lo. A raz√£o pela qual precisamos dobrar √© que nossa hip√≥tese nula foi a de Œº = 0. Ou seja, o nulo ser√° falso se nosso resultado for significativamente maior que zero (uma atitude positiva) ou significativamente menor que zero (uma atitude negativa). Isso √© consistente com a forma como fizemos nossa pergunta: \"as pessoas t√™m atitudes positivas ou negativas?\" Em outras palavras, n√£o testamos uma previs√£o direcional; estar√≠amos interessados ‚Äã‚Äãem \"encontrar\" algo, independentemente da dire√ß√£o do efeito. Como o valor p √© a probabilidade de obter um efeito \"desse tamanho\" e n√£o nos importamos com a dire√ß√£o, ele realmente existe nos dois lados da distribui√ß√£o (uma atitude negativa nos daria um escore t negativo):\n___\n\n![](img/unnamed-chunk-11-1.png)\n\nThus, we have to double our *p*-value. This is standard practice any\ntime you would be willing to declare the result significant **regardless\nof the direction**. We call this a *two-tailed p-value*.\n___\nPortanto, temos que dobrar nosso valor-p. Essa √© uma pr√°tica padr√£o sempre que voc√™ estiver disposto a declarar o resultado significativo, independentemente da dire√ß√£o. Chamamos isso de um valor p bicaudal.\n___\nIf this explanation is confusing, you can also understand it a slightly\ndifferent way: by testing *H*<sub>0</sub>‚ÄÑ:‚ÄÑ*Œº*‚ÄÑ=‚ÄÑ0, you are really\nasking whether *Œº*‚ÄÑ&lt;‚ÄÑ0 or whether *Œº*‚ÄÑ&gt;‚ÄÑ0. You are essentially\nasking two separate questions of the data. You need to double your\n*p*-value.\n\n___\n\nSe essa explica√ß√£o √© confusa, voc√™ tamb√©m pode entend√™-la de uma maneira um pouco diferente: ao testar H0: Œº = 0, voc√™ est√° realmente perguntando se Œº <0 ou se Œº> 0. Voc√™ est√° essencialmente fazendo duas perguntas separadas dos dados. Voc√™ precisa dobrar seu valor-p.\n___\nThis is almost always what you want. We almost always want to be able to\ndeclare a result significant if the effect is large, regardless of\nwhether the direction of the result matches our intuition or not. For\nexample, if an intervention to increase productivity backfires and\ndecreases productivity, we want to know that just as much as we want to\nknow if it works.\n___\n\nIsso √© quase sempre o que voc√™ deseja. Quase sempre queremos declarar um resultado significativo se o efeito for grande, independentemente de a dire√ß√£o do resultado corresponder √† nossa intui√ß√£o ou n√£o. Por exemplo, se uma interven√ß√£o para aumentar a produtividade sai pela culatra e diminui a produtividade, queremos saber disso tanto quanto queremos saber se funciona.\n___\nThus, we almost always double the *p*-value for this reason. It is true\nthat it makes it a little harder to get a significant result (less than\n.05), but we can extract more meaning from the result. It's worth it.\n\nNote: our doubled *p*-value here is still essentially zero:\n___\n\nAssim, quase sempre dobramos o valor de p por esse motivo. √â verdade que torna um pouco mais dif√≠cil obter um resultado significativo (menor que 0,05), mas podemos extrair mais significado do resultado. Vale a pena.\n\nNota: nosso valor de p duplicado aqui ainda √© essencialmente zero:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "2.0 * (1 - t.cdf(10.8, df = 99, loc=0, scale=1))",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}